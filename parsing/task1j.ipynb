{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64\",\n",
    "    \"Accept-Language\": \"ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7\"\n",
    "}\n",
    "r = requests.get('http://httpbin.org/get', headers=headers)\n",
    "\n",
    "print (r.status_code)\n",
    "# print (r.headers)\n",
    "# print (r.content)\n",
    "print (r.text)\n",
    "# print (r.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64\",\n",
    "    \"Accept-Language\": \"ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7\"\n",
    "}\n",
    "query = {\n",
    "    \"search_query\": \"весна\"\n",
    "} \n",
    "\n",
    "r = requests.get('http://www.youtube.com/results', headers=headers, params = query)\n",
    "\n",
    "print (r.status_code)\n",
    "# print (r.headers)\n",
    "# print (r.content)\n",
    "print (r.text)\n",
    "# print (r.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64\",\n",
    "    \"Accept-Language\": \"ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7\"\n",
    "}\n",
    "query = {\n",
    "    \"search_query\": \"весна\"\n",
    "} \n",
    "data = {\n",
    "    \"comments\": \"Test_request\", \n",
    "    \"custemail\": \"123465798@mail.com\", \n",
    "    \"custname\": \"ivan\", \n",
    "    \"custtel\": \"123456789\", \n",
    "    \"delivery\": \"\"\n",
    "}\n",
    "\n",
    "r = requests.post('http://httpbin.org/post', headers=headers, data = data)\n",
    "\n",
    "# print (r.status_code)\n",
    "# print (r.headers)\n",
    "# print (r.content)\n",
    "# print (r.text)\n",
    "print (r.json()['form'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://www.shutterstock.com/shutterstock/videos/1078354736/preview/stock-footage-drone-video-of-sunflower-field-in-a-beautiful-evening-sunset-k-aerial-view-of-sunflowers-in.webm'\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64\",\n",
    "    \"Accept-Language\": \"ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7\"\n",
    "}\n",
    "\n",
    "r = requests.get(url, headers=headers)\n",
    "\n",
    "# print (r)\n",
    "# print (r.headers)\n",
    "# print (r.content)\n",
    "print (r.text)\n",
    "# print (r.json()['form'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://www.shutterstock.com/shutterstock/videos/1078354736/preview/stock-footage-drone-video-of-sunflower-field-in-a-beautiful-evening-sunset-k-aerial-view-of-sunflowers-in.webm'\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64\",\n",
    "    \"Accept-Language\": \"ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7\"\n",
    "}\n",
    "\n",
    "r = requests.get(url, headers=headers, stream=True)\n",
    "\n",
    "# print (r)\n",
    "# print (r.headers)\n",
    "# print (r.content)\n",
    "# print (r.text)\n",
    "# print (r.json()['form'])\n",
    "# print (r.raw.read(10))\n",
    "with open ('1.webm', 'wb') as fd:\n",
    "    for chunk in r.iter_content(chunk_size=1024*100):\n",
    "        fd.write(chunk)\n",
    "        print ('write chunk 100 kb')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mozilla/4.0 (Windows; MSIE 7.0; Windows NT 5.1; SV1; .NET CLR 2.0.50727)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import fake_useragent\n",
    "\n",
    "ua = fake_useragent.UserAgent( )\n",
    "url = 'https://img.freepik.com/premium-photo/cargo-ship-sailing-on-the-sea-top-view-from-drone_44353-1806.jpg?w=996'\n",
    "\n",
    "def get_name (file_url):\n",
    "    return file_url.split('/')[-1].split('?')[0]\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": ua.random,\n",
    "    \"Accept-Language\": \"ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7\"\n",
    "}\n",
    "# res = requests.get(url, headers=headers, stream=True)\n",
    "# with open (get_name(url), 'wb') as fd:\n",
    "    # for chunk in r.iter_content(chunk_size=1024*100):\n",
    "        # fd.write(chunk)\n",
    "        # print ('write chunk 100 kb')\n",
    "# print (res.json()['headers'])\n",
    "\n",
    "res = requests.get(url, headers=headers)\n",
    "with open (get_name(url), 'wb') as fd:\n",
    "    fd.write(res.content)\n",
    "print (headers[\"User-Agent\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup('<a></b></a>', 'lxml')\n",
    "\n",
    "\n",
    "with open ('курсы/parsing/html/page.html', 'r') as f:\n",
    "    soup = BeautifulSoup(f, 'lxml')\n",
    "\n",
    "title = soup.title\n",
    "#.name - имя\n",
    "#.attrs - словарь с атрибутами\n",
    "#.text- содержание\n",
    "# .get_text()\n",
    "#.string\n",
    "print(title.text)\n",
    "# print (soup)\n",
    "print(soup.a) #получить ссылку, элемент с тегом а\n",
    "\n",
    "h1 = soup.h1\n",
    "\n",
    "print(h1.attrs)\n",
    "print(h1.attrs['id'])\n",
    "print(h1['id'])\n",
    "print(h1.get('id'))     # значение атрибута, None если нет значения\n",
    "print(h1.has_attr('id')) # проверить наличие атрибута\n",
    "\n",
    "print(h1.text)\n",
    "print(h1.text.strip())\n",
    "\n",
    "print(h1.get_text(strip=True, separator = '|'))\n",
    "print(h1.get_text(strip=True, separator = ' '))\n",
    "\n",
    "# методы find() - находит один элемент, find_all() - вернёт список всех найденых элементов\n",
    "print (soup.find('a'))  # аналогично print(soup.a)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'курсы/parsing/html/page.html'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [25], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbs4\u001b[39;00m \u001b[39mimport\u001b[39;00m BeautifulSoup\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m (\u001b[39m'\u001b[39;49m\u001b[39mкурсы/parsing/html/page.html\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m      5\u001b[0m     soup \u001b[39m=\u001b[39m BeautifulSoup(f, \u001b[39m'\u001b[39m\u001b[39mlxml\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m links \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39mfind_all(\u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'курсы/parsing/html/page.html'"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "with open ('курсы/parsing/html/page.html', 'r') as f:\n",
    "    soup = BeautifulSoup(f, 'lxml')\n",
    "\n",
    "links = soup.find_all('a')\n",
    "print (len(links))\n",
    "# print (soup.find_all('nav'))\n",
    "print (soup.find('nav').find('a').text)\n",
    "print (soup.find('nav').find('a').get('href'))\n",
    "\n",
    "links = soup.find('nav').find_all('a')\n",
    "print (len(links))\n",
    "for link in links:\n",
    "    print(f'{link.get_text(strip=True)} - {link.get(\"href\")}') # текст ссылки и её анкор\n",
    "    # print(f'{link.get(\"href\")}')\n",
    "    # print (f'{link.get_text(strip=True)} - {link.get('href')}')\n",
    "\n",
    "# links = soup.find('nav').find_all('a', class_='p-2 link-secondary')\n",
    "# print (links)\n",
    "\n",
    "# links = soup.find('nav').find_all('a', attrs = {'class': 'p-2 link-secondary active'})\n",
    "# print (links)\n",
    "\n",
    "\n",
    "# links = soup.find('nav').find_all('a', attrs = {'data-link': \"link\",\n",
    "                                                # 'class': 'p-2 link-secondary'})\n",
    "# print (links)\n",
    "\n",
    "# links = soup.find('nav').find_all(id = 'design')\n",
    "# print (links)\n",
    "\n",
    "# print (soup.find_all(['h1','h2','h3','h4']))\n",
    "\n",
    "\n",
    "# print (soup.find_all(re.compile('^h[1-6]'))) # регультрное выражение вместо списка\n",
    "\n",
    "\n",
    "print (soup.find_all(True)[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\egors\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\certifi\\cacert.pem\n"
     ]
    }
   ],
   "source": [
    "import certifi\n",
    "\n",
    "print(certifi.where())\n",
    "# расположение файла с сертификатами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "with open ('курсы/parsing/html/page.html', 'r') as f:\n",
    "    soup = BeautifulSoup(f, 'lxml')\n",
    "\n",
    "links = soup.find_all('a')\n",
    "print (len(links))\n",
    "# print (soup.find_all('nav'))\n",
    "print (soup.find('nav').find('a').text)\n",
    "print (soup.find('nav').find('a').get('href'))\n",
    "\n",
    "links = soup.find('nav').find_all('a')\n",
    "print (len(links))\n",
    "for link in links:\n",
    "    print(f'{link.get_text(strip=True)} - {link.get(\"href\")}') # текст ссылки и её анкор\n",
    "    # print(f'{link.get(\"href\")}')\n",
    "    # print (f'{link.get_text(strip=True)} - {link.get('href')}')\n",
    "\n",
    "# links = soup.find('nav').find_all('a', class_='p-2 link-secondary')\n",
    "# print (links)\n",
    "\n",
    "# links = soup.find('nav').find_all('a', attrs = {'class': 'p-2 link-secondary active'})\n",
    "# print (links)\n",
    "\n",
    "\n",
    "# links = soup.find('nav').find_all('a', attrs = {'data-link': \"link\",\n",
    "                                                # 'class': 'p-2 link-secondary'})\n",
    "# print (links)\n",
    "\n",
    "# links = soup.find('nav').find_all(id = 'design')\n",
    "# print (links)\n",
    "\n",
    "# print (soup.find_all(['h1','h2','h3','h4']))\n",
    "\n",
    "\n",
    "# print (soup.find_all(re.compile('^h[1-6]'))) # регультрное выражение вместо списка\n",
    "\n",
    "\n",
    "print (soup.find_all(True)[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import soupsieve as sv\n",
    "\n",
    "with open('html/page.html', 'r') as f:\n",
    "    soup = BeautifulSoup(f, 'lxml')\n",
    "\n",
    "# links = soup.find('nav', class_='nav').find_all('a')\n",
    "# print(links)\n",
    "\n",
    "# links = soup.select_one('nav').select('a')\n",
    "# print(links)\n",
    "\n",
    "# links = sv.select('nav a', soup)\n",
    "# print(links)\n",
    "\n",
    "# links = sv.select('nav > span', soup)\n",
    "# print(links)\n",
    "\n",
    "# links = sv.select('nav a:is(.test1, .test2)', soup)\n",
    "# print(links)\n",
    "\n",
    "# links = sv.select('nav a:not(.test1, .test2)', soup)\n",
    "# print(links)\n",
    "\n",
    "# links = sv.select('nav a[data-link]', soup)\n",
    "# print(links)\n",
    "\n",
    "# links = sv.select('nav a[href=\"#\"]', soup)\n",
    "# print(links)\n",
    "\n",
    "# links = sv.select('nav.nav a[href!=\"#\"]', soup)\n",
    "# print(links)\n",
    "\n",
    "# links = sv.select('nav.nav a:not([href=\"#\"])', soup)\n",
    "# print(links)\n",
    "\n",
    "# table_names = sv.select('table.table tbody tr td:first-child', soup)\n",
    "# print(table_names)\n",
    "#\n",
    "# table_upvotes = sv.select('table.table tbody tr td:nth-child(2)', soup)\n",
    "# print(table_upvotes)\n",
    "#\n",
    "# table_downvotes = sv.select('table.table tbody tr td:last-child', soup)\n",
    "# print(table_downvotes)\n",
    "\n",
    "el = soup.find('table', class_='table').find_parent().select('a')\n",
    "print(el)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = re.compile(\"Одежда\")\n",
    "soap.find_all(text = re.compile(\"([Оо]дежда)\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ac9b9e84abeb210f4ed304763a3407f98a7d6f7b23eb0ef529c957140bb3657f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
